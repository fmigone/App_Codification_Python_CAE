{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Camembert_With_TF.ipynb",
      "provenance": [],
      "mount_file_id": "1CtKJJcRGtXlPTZzTvP0wIB4VIvkt6q5m",
      "authorship_tag": "ABX9TyOaWVfwTniclvwLkjwm3aif",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmigone/App_Codification_Python_CAE/blob/main/Camembert_With_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Préambule"
      ],
      "metadata": {
        "id": "TmeiY8PEpxer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installation des packages\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "3nrggamPaedl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PtCNxYDXUmq"
      },
      "outputs": [],
      "source": [
        "#Importation de packages\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MultiLabelBinarizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "from transformers import (\n",
        "    CamembertTokenizer,\n",
        "    TFCamembertModel,\n",
        "    TFRobertaPreTrainedModel,\n",
        "    TFRobertaMainLayer\n",
        ")\n",
        "from transformers.modeling_tf_utils import get_initializer\n",
        "from tensorflow.keras.layers import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyper_Parameter\n",
        "BATCH_SIZE = np.array([16,24,32])\n",
        "STEPS = 32\n",
        "EPOCHS = 10\n",
        "LR = np.array([1e-05,2e-05,3e-05])\n",
        "LR_DECAY = \"Linear\"\n",
        "WD = 0.01\n",
        "WR = 0.06\n",
        "Adam_epsilon = 1e-6 \n",
        "Adam_beta = 0.9 \n",
        "Adam_beta = 0.98 "
      ],
      "metadata": {
        "id": "-cf2Z8m0Xi2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation des données"
      ],
      "metadata": {
        "id": "TbQiLAUBqCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chargement des données\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Base_CITI_Produits_REVU.xlsx\")"
      ],
      "metadata": {
        "id": "2TISx15W7uxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Traitement\n",
        "df = df[['Libelle', 'CODE']]\n",
        "df['CODE'] = df['CODE'].astype(int)\n",
        "num_classes = len(df['CODE'].value_counts())"
      ],
      "metadata": {
        "id": "mnvDrnwQ8lRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Pretrained Model end Embeddings"
      ],
      "metadata": {
        "id": "u1IumpOI1CTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization et Embeddings"
      ],
      "metadata": {
        "id": "IAI893IeqGx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "vFqwOEg7qKu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "njMTUtudpvMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings"
      ],
      "metadata": {
        "id": "ShFPMUAdqN1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sfYgmj4KqSat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelisation"
      ],
      "metadata": {
        "id": "KksYKkx7qqWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect"
      ],
      "metadata": {
        "id": "wGPRjakQ4-UJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TFRobertaForSequenceClassification"
      ],
      "metadata": {
        "id": "nx98VMBS5ZZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_DF = inspect.getsource(TFRobertaForSequenceClassification)\n",
        "print(source_DF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YUFqAVo5Cyb",
        "outputId": "7f48784c-6eac-46fa-a74b-335b7dda3d1d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class TFRobertaForSequenceClassification(TFRobertaPreTrainedModel, TFSequenceClassificationLoss):\n",
            "    # names with a '.' represents the authorized unexpected/missing layers when a TF model is loaded from a PT model\n",
            "    _keys_to_ignore_on_load_unexpected = [r\"pooler\", r\"lm_head\"]\n",
            "\n",
            "    def __init__(self, config, *inputs, **kwargs):\n",
            "        super().__init__(config, *inputs, **kwargs)\n",
            "        self.num_labels = config.num_labels\n",
            "\n",
            "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\n",
            "        self.classifier = TFRobertaClassificationHead(config, name=\"classifier\")\n",
            "\n",
            "    @unpack_inputs\n",
            "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
            "    @add_code_sample_docstrings(\n",
            "        processor_class=_TOKENIZER_FOR_DOC,\n",
            "        checkpoint=\"cardiffnlp/twitter-roberta-base-emotion\",\n",
            "        output_type=TFSequenceClassifierOutput,\n",
            "        config_class=_CONFIG_FOR_DOC,\n",
            "        expected_output=\"'optimism'\",\n",
            "        expected_loss=0.08,\n",
            "    )\n",
            "    def call(\n",
            "        self,\n",
            "        input_ids: Optional[TFModelInputType] = None,\n",
            "        attention_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
            "        token_type_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
            "        position_ids: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
            "        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
            "        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
            "        output_attentions: Optional[bool] = None,\n",
            "        output_hidden_states: Optional[bool] = None,\n",
            "        return_dict: Optional[bool] = None,\n",
            "        labels: Optional[Union[np.ndarray, tf.Tensor]] = None,\n",
            "        training: Optional[bool] = False,\n",
            "    ) -> Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]:\n",
            "        r\"\"\"\n",
            "        labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n",
            "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
            "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
            "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
            "        \"\"\"\n",
            "        outputs = self.roberta(\n",
            "            input_ids,\n",
            "            attention_mask=attention_mask,\n",
            "            token_type_ids=token_type_ids,\n",
            "            position_ids=position_ids,\n",
            "            head_mask=head_mask,\n",
            "            inputs_embeds=inputs_embeds,\n",
            "            output_attentions=output_attentions,\n",
            "            output_hidden_states=output_hidden_states,\n",
            "            return_dict=return_dict,\n",
            "            training=training,\n",
            "        )\n",
            "        sequence_output = outputs[0]\n",
            "        logits = self.classifier(sequence_output, training=training)\n",
            "\n",
            "        loss = None if labels is None else self.hf_compute_loss(labels, logits)\n",
            "\n",
            "        if not return_dict:\n",
            "            output = (logits,) + outputs[2:]\n",
            "            return ((loss,) + output) if loss is not None else output\n",
            "\n",
            "        return TFSequenceClassifierOutput(\n",
            "            loss=loss,\n",
            "            logits=logits,\n",
            "            hidden_states=outputs.hidden_states,\n",
            "            attentions=outputs.attentions,\n",
            "        )\n",
            "\n",
            "    # Copied from transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification.serving_output\n",
            "    def serving_output(self, output: TFSequenceClassifierOutput) -> TFSequenceClassifierOutput:\n",
            "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\n",
            "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\n",
            "\n",
            "        return TFSequenceClassifierOutput(logits=output.logits, hidden_states=hs, attentions=attns)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFRobertaForSequenceClassification,TFCamembertForSequenceClassification, CamembertTokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "EdEwBw814vnd",
        "outputId": "cb741de2-d224-4616-a403-672e418ab3cf"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-062505045ebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFRobertaClassificationHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFRobertaForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTFCamembertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCamembertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'TFRobertaClassificationHead' from 'transformers' (/usr/local/lib/python3.7/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xLaGUgtQtV8D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TFCamembertForSequenceClassification(TFRobertaPreTrainedModel):\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super().__init__(config, *inputs, **kwargs)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = TFRobertaMainLayer(config, name=\"roberta\")\n",
        "        self.classifier = TFRobertaClassificationHead(config, name=\"classifier\")\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        outputs = self.roberta(inputs, **kwargs)\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output, training=kwargs.get(\"training\", False))\n",
        "        outputs = (logits,) + outputs[2:]\n",
        "        return outputs  # logits, (hidden_states), (attentions)"
      ],
      "metadata": {
        "id": "CwfL4ebJ1Oat"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TFCamembertForSequenceClassification.from_pretrained(\"jplu/tf-camembert-base\",num_labels=num_classes)\n",
        "tokenizer = CamembertTokenizer.from_pretrained(\n",
        "    \"jplu/tf-camembert-base\",\n",
        "    output_hidden_states=True,\n",
        "    output_attentions=True\n",
        ")"
      ],
      "metadata": {
        "id": "vHDkcyfX1br6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation du modèle pré-entrainé"
      ],
      "metadata": {
        "id": "oGFEH4Xbq9Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KJtU2gT0tSho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recherche des Hyper-Paramètres"
      ],
      "metadata": {
        "id": "nBZbSefhrGYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrainement"
      ],
      "metadata": {
        "id": "TYTzJ1orqvUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exportation"
      ],
      "metadata": {
        "id": "O8jUxPqUqyzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nUPbEDMjqu0n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}